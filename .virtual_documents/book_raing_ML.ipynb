import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as ms


df = pd.read_csv("books.csv",sep=",") #,index_col="bookID"


# Replace 'your_detected_encoding' with the actual encoding detected, e.g., 'utf-8'
df = pd.read_csv('books.csv', index_col="bookID", encoding='utf-8')

# Print the DataFrame to verify that the text is readabl
print(df.head())


df.isnull().values.any()


type(df)


df.head(10)


df.shape


df.info()





df.ndim


df.describe()


df.columns[:]


df.loc[16914]


#df['authors'].replace("David E. Smith (Turgon of TheOneRing.net","David E. Smith",inplace=True)


df.loc[16914]


df['average_rating'].replace(" one of the founding members of this Tolkien website)/Verlyn Flieger/Turgon (=David E. Smith)",np.nan,inplace=True)
df['authors'].replace("David E. Smith (Turgon of TheOneRing.net","David E. Smith",inplace=True)


df.loc[16914]


df.info()


#concatonate authors from col. authors and average_rating
combined_value1 = df.loc[12224,'authors']+df.loc[12224,'average_rating']
df.loc[12224,'authors'] = combined_value1


df.loc[16914]


#combined_value2 = df.loc[16914,'authors']+df.loc[16914,'average_rating']
#df.loc[16914,'authors'] = combined_value2
combined_value3 = str(df.loc[22128,'authors'])+","+str(df.loc[22128,'average_rating'])
df.loc[22128,'authors'] = combined_value3


combined_value4 = str(df.loc[34889,'authors'])+str(df.loc[34889,'average_rating'])
df.loc[34889,'authors'] = combined_value4


#replace the unfinished author list with the concatonated values aboved
df.loc[12224,'authors'] = combined_value1
#df.loc[16914,'authors'] = combined_value2
df.loc[22128,'authors'] = combined_value3
df.loc[34889,'authors'] = combined_value4


#readjust misaligned columns values by shifting all columns to the left
df.loc[12224,'average_rating'] = df.loc[12224,'isbn']
df.loc[12224,'isbn'] = df.loc[12224,'isbn13']
df.loc[12224,'isbn13'] = df.loc[12224,'language_code']
df.loc[12224,'language_code'] = df.loc[12224,'num_pages']
df.loc[12224,'num_pages'] = df.loc[12224,'ratings_count']
df.loc[12224,'ratings_count'] = df.loc[12224,'text_reviews_count']
df.loc[12224,'text_reviews_count'] = df.loc[12224,'publication_date']
df.loc[12224,'publication_date'] = df.loc[12224,'publisher']
df.loc[12224,'publisher'] = df.loc[12224,'unnamed']

df.loc[16914,'average_rating'] = df.loc[16914,'isbn']
df.loc[16914,'isbn'] = df.loc[16914,'isbn13']
df.loc[16914,'isbn13'] = df.loc[16914,'language_code']
df.loc[16914,'language_code'] = df.loc[16914,'num_pages']
df.loc[16914,'num_pages'] = df.loc[16914,'ratings_count']
df.loc[16914,'ratings_count'] = df.loc[16914,'text_reviews_count']
df.loc[16914,'text_reviews_count'] = df.loc[16914,'publication_date']
df.loc[16914,'publication_date'] = df.loc[16914,'publisher']
df.loc[16914,'publisher'] = df.loc[16914,'unnamed']

df.loc[22128,'average_rating'] = df.loc[22128,'isbn']
df.loc[22128,'isbn'] = df.loc[22128,'isbn13']
df.loc[22128,'isbn13'] = df.loc[22128,'language_code']
df.loc[22128,'language_code'] = df.loc[22128,'num_pages']
df.loc[22128,'num_pages'] = df.loc[22128,'ratings_count']
df.loc[22128,'ratings_count'] = df.loc[22128,'text_reviews_count']
df.loc[22128,'text_reviews_count'] = df.loc[22128,'publication_date']
df.loc[22128,'publication_date'] = df.loc[22128,'publisher']
df.loc[22128,'publisher'] = df.loc[22128,'unnamed']

df.loc[34889,'average_rating'] = df.loc[34889,'isbn']
df.loc[34889,'isbn'] = df.loc[34889,'isbn13']
df.loc[34889,'isbn13'] = df.loc[34889,'language_code']
df.loc[34889,'language_code'] = df.loc[34889,'num_pages']
df.loc[34889,'num_pages'] = df.loc[34889,'ratings_count']
df.loc[34889,'ratings_count'] = df.loc[34889,'text_reviews_count']
df.loc[34889,'text_reviews_count'] = df.loc[34889,'publication_date']
df.loc[34889,'publication_date'] = df.loc[34889,'publisher']
df.loc[34889,'publisher'] = df.loc[34889,'unnamed']


df.loc[12224]


df.loc[22128]


df.loc[34889]


df.loc[16914]


df.drop(columns='unnamed',axis=1,inplace=True)


df.info()


df.head(2)


df.isna().sum()


ms.bar(df);





df.info()


# # Identify and print string values in the 'num_pages' column
# str_values = df['num_pages'][df['num_pages'].apply(lambda x: isinstance(x, str))]

# print("\nString values in 'num_pages' column:")
# print(str_values)


# # Identify and print string values in the 'num_pages' column
# int_values = df['num_pages'][df['num_pages'].apply(lambda x: isinstance(x, int))]

# print("\nInteger values in 'num_pages' column:")
# print(int_values)


# # Identify and print string values in the 'num_pages' column
# str_values = df['text_reviews_count'][df['text_reviews_count'].apply(lambda x: isinstance(x, str))]

# print("\nString values in 'text_reviews_count' column:")
# print(str_values)


# getting unique value from 'year_of_publication' feature
df['publication_date'].unique()


type(df.loc[2]['num_pages'])


print(df.loc[2, 'num_pages'])


type(df.loc[2]['text_reviews_count'])


type(df.loc[12224]['text_reviews_count'])


print(df.loc[2]['text_reviews_count'])


print(df.loc[12224, 'num_pages'])


print(df.loc[12224]['text_reviews_count'])


#value = df.loc[12224]['text_reviews_count']
# Simplified function to convert string numbers to integers, non-numeric to 0
def convert_to_int(str_values):
    try:
        return int(str_values)
    except ValueError:
        return 0

# Apply the conversion function to the entire 'text_reviews_count' column
df['text_reviews_count'] = df['text_reviews_count'].apply(convert_to_int)

# Print the DataFrame after conversion
print("\nDataFrame after conversion:")
type(df.loc[12224]['text_reviews_count'])


df.loc[12224]


df['num_pages'] = df['num_pages'].astype(int)


# Detect rows where 'num_pages' cannot be converted to a numeric value
non_numeric_rows = df[pd.to_numeric(df['num_pages'], errors='coerce').isna()]

# Print the rows with non-numeric 'num_pages' values
print("Rows with non-numeric 'num_pages' values:")
print(non_numeric_rows)


df.isna().sum()


df['language_code'].value_counts()





#count all English variation as english
#df['language_code'] = df['language_code'].replace(['en-US','en-GB','en-CA','enm'],'eng')


df['language_code'].value_counts()


#df = df.drop(columns=['isbn','isbn13'])


df.head(10)


df['publisher'].value_counts()


# Find all unique types in the 'num_pages' column
unique_types = df['average_rating'].apply(type).unique()

# Print the unique types
print("Unique types in 'average_rating' column:")
for t in unique_types:
    print(t)


df.iloc[3340:3350]


df.iloc[5877]


# Convert 'average_rating' to numeric
df['average_rating'] = pd.to_numeric(df['average_rating'])

type(df.loc[2]['average_rating'])






import pandas as pd

# Assuming 'df' is your DataFrame

# Convert 'average_rating' column to float
df['average_rating'] = df['average_rating'].astype(float)

# Verify the conversion by finding all unique types in the 'average_rating' column
unique_types = df['average_rating'].apply(type).unique()

# Print the unique types
print("Unique types in 'average_rating' column:")
for t in unique_types:
    print(t)

# Group by publisher and calculate the average rating
publisher_ratings = df.groupby('publisher')['average_rating'].mean().reset_index()

# Sort publishers by average rating in descending order
publisher_ratings = publisher_ratings.sort_values(by='average_rating', ascending=False)

# Print the top publishers by average rating
print(publisher_ratings.head(10))



# import chardet

# # Detect the encoding of a file
# with open('/Users/tanattiya/Desktop/DSTI/ML/Project1/books.csv', 'rb') as f:
#     result = chardet.detect(f.read())

# print(result)



#print(df[df['bookID'] == 22802])
df.iloc[6076:6090]


print(df.columns)



df.loc[22802:22810]


#far left's indexing starts from 0
df.iloc[1]


#position of the index of the column set as index_col (this case = bookID)
df.loc[1]


type(df.loc[2]['average_rating'])






# # Calculate the average rating for each publisher
# publisher_ratings = df.groupby('publisher')['average_rating'].mean().reset_index()

# # Sort publishers by average rating in descending order and select the top 50
# top_publishers = publisher_ratings.sort_values(by='average_rating', ascending=False).head(50)

# # Plot the average ratings by publisher for the top 50
# plt.figure(figsize=(10, 8))
# sns.barplot(x='average_rating', y='publisher', data=top_publishers)
# plt.title('Average Book Rating by Publisher (Top 50)')
# plt.xlabel('Average Rating')
# plt.ylabel('Publisher')
# plt.show()

# # Filter the original DataFrame to include only the top 50 publishers
# df_filtered = df[df['publisher'].isin(top_publishers['publisher'])]

# # Perform ANOVA test to see if the differences in ratings are statistically significant
# grouped_ratings = [group['average_rating'].values for name, group in df_filtered.groupby('publisher')]
# anova_result = f_oneway(*grouped_ratings)

# print(f"ANOVA test result: F={anova_result.statistic}, p={anova_result.pvalue}")


numerical_features = df.select_dtypes(exclude=["object"])


numerical_features


numerical_features.info()


sns.heatmap(numerical_features.corr(),annot=True,center=True);


for feature in numerical_features:
    plt.figure(figsize=(8,5))
    sns.boxplot(x=numerical_features[feature]);


for feature in numerical_features:
    plt.figure(figsize=(8,5))
    sns.distplot(numerical_features[feature],bins=5);


target = numerical_features["average_rating"]


for feature in numerical_features:
    plt.figure(figsize=(8,5))
    sns.regplot(x=numerical_features[feature],y=target,data=numerical_features);


df.language_code.value_counts().plot.bar();
#(autopct="%.1f%%",shadow=True,legend=True,figsize=(15,8));


#df.publisher.value_counts().plot.bar(figsize=(20,6));


# # Let's look at the top 10 rated books
# top10Books = df.nlargest(10, ['ratings_count']).set_index('title')['ratings_count']
# plot_dims = (12, 8)
# fig, ax = plt.subplots(figsize=plot_dims)
# sns.barplot(top10Books, top10Books.index)

# for i in ax.patches:
#     ax.text(i.get_width()+.3, i.get_y()+0.5, str(round(i.get_width())), fontsize = 15, color = 'k')
    
# plt.show()


df.query("authors=='Bill Bryson'")

# Select the top 10 books using query function in combination with 
top_10_books = df.nlargest(10, 'average_rating').query('index >= 0')

top_10_books


df[["title","average_rating"]].query("average_rating==5.0")


for i in df.columns:
    print(i,df[i].duplicated().sum())


df.nunique()


775+10352


df[df.authors=="Don DeLillo"].count()


df.duplicated().sum()


df_time_series = numerical_features.copy()
df_time_series.head()


df_time_series["date"]=df["publication_date"]
df_time_series.head()


df_time_series.info()


# If you know the format of your dates, specify it
# Example format: 'dd/mm/yyyy' or 'yyyy-mm-dd'
date_format = '%m/%d/%Y'  # Adjust this format to your needs
df_time_series["date"] = pd.to_datetime(df_time_series["date"], format=date_format, errors='coerce')


df_time_series.info()


df_time_series["date"].isna().sum()


4555+6572


df_time_series.info()


df_time_series.head()


df_time_series.set_index(df_time_series["date"],inplace=True)
df_time_series.sort_index(inplace=True)


df_time_series.head()


df_time_series.drop("date",axis=1,inplace=True)
df_time_series.head()


df_time_series["average_rating"].resample('M').mean().plot();


df_time_series["average_rating"].resample('2W').std().plot();


df_time_series["num_pages"].resample('M').mean().plot();


df_time_series["ratings_count"].resample('Y').mean().plot();


df_time_series["average_rating"].resample('W').agg(["mean","std","median"]).plot();


df_time_series["text_reviews_count"].resample('Y').agg(["mean","std","median"]).plot();


from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer, silhouette_visualizer
from sklearn.cluster import KMeans


kms=KMeans(random_state=42,n_init='auto')
viz=KElbowVisualizer(kms,k=(1,20))
viz.fit(numerical_features)
viz.show();


model=KMeans(n_clusters=3,random_state=42)
y_prediction=model.fit_predict(numerical_features)


y_prediction


sns.histplot(x=y_prediction);


numerical_features["clusters"]=y_prediction
numerical_features.head()


numerical_features["clusters"].value_counts().plot.bar().legend();


df0=numerical_features[numerical_features.clusters==0]
df1=numerical_features[numerical_features.clusters==1]
df2=numerical_features[numerical_features.clusters==2]


plt.scatter(df0.num_pages,df0.average_rating,color="red",label="cluster 0")
plt.scatter(df1.num_pages,df1.average_rating,color="orange",label="cluster 1")
plt.scatter(df2.num_pages,df2.average_rating,color="yellow",label="cluster 2")
plt.scatter(model.cluster_centers_[:,1],model.cluster_centers_[:,0],s=100,marker="*",label="centroids",color="black")
plt.xlabel("number of pages")
plt.ylabel("average rating")
plt.legend()
plt.show()


plt.scatter(df0.ratings_count,df0.average_rating,color="red",label="cluster 0")
plt.scatter(df1.ratings_count,df1.average_rating,color="orange",label="cluster 1")
plt.scatter(df2.ratings_count,df2.average_rating,color="yellow",label="cluster 2")
plt.scatter(model.cluster_centers_[:,2],model.cluster_centers_[:,0],s=100,marker="*",label="centroids",color="black")
plt.xlabel("ratings count")
plt.ylabel("average rating")
plt.legend()
plt.show()


plt.scatter(df0.text_reviews_count,df0.average_rating,color="red",label="cluster 0")
plt.scatter(df1.text_reviews_count,df1.average_rating,color="orange",label="cluster 1")
plt.scatter(df2.text_reviews_count,df2.average_rating,color="yellow",label="cluster 2")
plt.scatter(model.cluster_centers_[:,3],model.cluster_centers_[:,0],s=100,marker="*",label="centroids",color="black")
plt.xlabel("text reviews count")
plt.ylabel("average rating")
plt.legend()
plt.show()


X=numerical_features.drop("clusters",axis=1)
#silhouette_visualizer(model, df_silhouette, colors='yellowbrick')


from yellowbrick.cluster import SilhouetteVisualizer, silhouette_visualizer
from sklearn.cluster import KMeans


vz = SilhouetteVisualizer(model,colors='yellowbrick')
vz.fit(X)
vz.show();


df.head(10)


from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, RobustScaler, PolynomialFeatures


df.info()


df.language_code.value_counts()


df.language_code.replace(["ara","nl","srp","msa","glg","wel","nor","tur","gla","ale","mul","zho","grc","por","en-CA","ita","enm","lat","rus","swe","jpn","ger"],"others", inplace=True)


df.language_code.value_counts()


df_language_code=pd.get_dummies(df["language_code"],dtype=int)
df_language_code


df_combined=pd.concat([df,df_language_code],axis=1)
df_combined.drop(["language_code","isbn","isbn13"],axis=1, inplace=True)


df_combined.head()


df_combined.info()


#date_format = '%m/%d/%Y'  # Adjust this format to your needs
#df_combined["publication_date"] = pd.to_datetime(df_combined["publication_date"], format=date_format, errors='coerce')


# Assuming df_combined is your DataFrame
# First, coerce errors to NaT to identify problematic entries
# Check for NaT values to identify problematic entries
problematic_rows = df_combined[df_combined["publication_date"].isna()]

# Inspect problematic rows to understand the issue
problematic_rows

# Further troubleshoot and correct the data as necessary


df_combined['publication_date'].head()


df_combined.info()


df_combined["publication_date"] = pd.to_datetime(df_combined["publication_date"], format="%m/%d/%Y", errors='coerce')


df_combined['publication_year'] = df_combined['publication_date'].dt.year
df_combined['publication_month'] = df_combined['publication_date'].dt.month
#df_combined[['publication_month','publication_year']] = df_combined[['publication_month','publication_year']].astype(int)


df_combined.head()


encoder=LabelEncoder()
df_combined.title=encoder.fit_transform(df_combined.title)
df_combined.authors=encoder.fit_transform(df_combined.authors)
df_combined.publisher=encoder.fit_transform(df_combined.publisher)


df_combined.head()


df_combined.info()


df_combined.drop("publication_date",axis=1,inplace=True)


# df_combined['publication_year'] = df_combined['publication_year'].astype('int')
# df_combined['publication_month'] = df_combined['publication_month'].astype('int')


# Assuming df_combined is your DataFrame
# First, coerce errors to NaT to identify problematic entries
# Check for NaT values to identify problematic entries
problematic_rows = df_combined[df_combined["publication_year"].isna()]

# Inspect problematic rows to understand the issue
problematic_rows

# Further troubleshoot and correct the data as necessary


df_combined['publication_year'].fillna(0000,axis=0,inplace=True)
df_combined['publication_month'].fillna(00,axis=0,inplace=True)


problematic_rows = df_combined[["publication_month","publication_year"]].isna()
ms.matrix(problematic_rows);


df_combined[["publication_month","publication_year"]]=df_combined[["publication_month","publication_year"]].astype(int)


df_combined.info()


X=df_combined.drop("average_rating",axis=1)
y=df_combined.average_rating


X


y


X.ndim


y.ndim


from yellowbrick.model_selection import FeatureImportances
from sklearn.linear_model import LinearRegression, SGDRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor


model1 = LinearRegression()
viz = FeatureImportances(model1)
viz.fit(X, y)
viz.show();


model1 = SGDRegressor()
viz = FeatureImportances(model1)
viz.fit(X, y)
viz.show();


model1 = DecisionTreeRegressor()
viz = FeatureImportances(model1)
viz.fit(X, y)
viz.show();


# !pip install lazypredict


# !pip install --upgrade scikit-learn


from lazypredict.Supervised import LazyRegressor


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train.shape, X_test.shape, y_train.shape, y_test.shape


8901 + 2226


X_train.ndim, X_test.ndim, y_train.ndim, y_test.ndim


reg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None)
models, predictions = reg.fit(X_train, X_test, y_train, y_test)

models


from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.pipeline import make_pipeline


model = make_pipeline(MinMaxScaler(), PolynomialFeatures(degree=2), HistGradientBoostingRegressor(random_state=42))
model.fit(X_train, y_train)
model.score(X_test, y_test)


y_predict = model.predict(X_test)


comparison_df= pd.DataFrame({'Actual': y_test, 'Predicted': y_predict, 'Residual': y_test - y_predict})


comparison_df
